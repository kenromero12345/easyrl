<!doctype html>
<html lang="en">
    <head>
        <!--needed for bootstrap-->
        <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet"
              integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
              crossorigin="anonymous">
        <link rel= "stylesheet" type= "text/css" href= "{{ url_for('static',filename='styles/styles.css') }}">
        <script type=text/javascript src="{{  url_for('static', filename='js/combined.js') }}"></script>
        <title>EasyRL</title>
  </head>
  <body>
    <!--    Navigation bar-->
      <nav class="navbar navbar-expand-lg navbar-light bg-light">
          <div class="container-fluid">
              <a class="navbar-brand" href="/index">EasyRL</a>
              <button
                  class="navbar-toggler"
                  type="button"
                  data-mdb-toggle="collapse"
                  data-mdb-target="#navbarNavAltMarkup"
                  aria-controls="navbarNavAltMarkup"
                  aria-expanded="false"
                  aria-label="Toggle navigation"
              >
                  <i class="fas fa-bars"></i>
              </button>
              <div class="collapse navbar-collapse" id="navbarNavAltMarkup">
                  <div class="navbar-nav">
                      <a class="nav-link" href="/index">Model</a>
                      <a class="nav-link" href="/about">About</a>
                      <a class="nav-link active" aria-current="page" href="/#">Help</a>
                  </div>
              </div>
          </div>
      </nav>

      <div class="helpContainer">
          <h3>
              Welcome to Portal RL
          </h3>
          <h5>A user friendly environment for learning and deploying reinforcement learning
              environments and agents
          </h5>

          <h4>
              MAIN WORK AREA
          </h4>
          <h5>
              Getting Started:
          </h5>
          <h6>
              1) Number of episodes
          </h6>
          <p>
              This parameter is used to define how many times the program should run the same
              simulation. Depending on the complexity of the model, a user might choose to run the simulation
              thousands of times to learn an optimal policy for the task it is trying to perform.
          </p>
          <p>
              NEW USER TIP:
              We recommend starting with a low number of episodes and gradually increasing the amount
              This will give you a sense of how reinforcement learning works and the affect
              the number of episodes has on various agent/environment interactions.
          </p>
          <h6>
              2) Max Steps
          </h6>
          <p>
              This number dictates how many actions will be taken during each episode before a training
              is concluded
          </p>
          <p>
              NEW USER TIP:
              Arbitrarily large numbers of steps don't always work out to better learning. Some of the
              suggested starting environments have a limited size. As you become more familiar with the
              environments you will get a feel for how many steps it should take to find an optimal policy
              and can adjust accordingly. You will learn that some reinforcement learning programs take a
              large number of episodes (which can include multiple days of training) and an ideal number of
              steps will allow an agent to run episodes efficiently.
          </p>
          <h6>
              3) Select Environment
          </h6>
          <p>
              This drop down menu will allow you to select a reinforcement learning environment
              from the available python gym sandboxes including a selection of atari games
          </p>
          <p>
              NEW USER TIP:
              We recommend starting with CartPole, CartPole Discrete, FrozenLake, Pendulum, Acrobat, or
              MountainCar. As you become more in tune with how these environments work move into the atari
              games, and if you feel inspired we hope you explore the internet for ways to develop your own
              environments and interact with our API (Found in our Advanced Options section).
          </p>
          <h6>
              4) Select Agent
          </h6>
          <p>
              This drop down menu contains several of the most studied reinforcement learning
              algorithms to match to your environment.
          </p>
          <p>
              NEW USER TIP:
              Don't worry, we have installed some guard rails to keep you from matching agents and
              environments that don't work together. As you study how reinforcement learning works you will
              understand why those combinations don't work together.
          </p>
          <h6>
              5) Set Model
          </h6>
          <p>
              Once you have selected an environment and an agent this will open the training interface.
          </p>
          <br>
          <h5>
            Training Interface
          </h5>
          <h6>
              1) Gamma
          </h6>
          <p>
              This value (0 <= X <= 1) represents the discount that the reinforcement learning agents assign to
              future rewards when calculating the value of a being between one action and the next. How this is
              done varies from algorithm to algorithm.
          </p>
          <p>
              NEW USER TIP:
              This value is, in almost all circumstances, very close to one.
          </p>
          <h6>
              2) Min/Max Epsilon and Decay Rate
          </h6>
          <p>
              This value represents the proportion of time an agent should choose a random action or consult
              its policy. A value of 1 means that it will always choose a random action and a value of 0 means it
              will always consult the policy. This is set to the Max value initially and decrements at
              intervals by the decay rate during training. When testing these should be set to zero.
          </p>
          <h6>
              3) Batch Size
          </h6>
          <p>
              When training a neural network this is the size of the group of actions and rewards that the
              network will consider each time while training its decision process. In any neural network this is
              a value that is fine tuned through testing.
          </p>
          <h6>
              4) Memory size
          </h6>
          <p>
              This is the maximimum number of state, action, and reward tuples that are stored for reference
              by the agent.
          </p>
          <h6>
              5) History Length
          </h6>
          <p>
              Neural networks often rely on information that is gained from its perception of the environment, and
              in environments that a single image doesn't tell the user much about what is happening in a given
              state this variable determines the number of chronological frames that a neural network will
              consider when updating its policy.
          </p>
          <h6>
              6) Alpha
          </h6>
          <p>
              Alpha is another name for the learning rate or step size. It defines the impact new information has
              when overwriting previous values.
          </p>
          <h6>
              7) Train
          </h6>
          <p>
              Locks in current parameters and initiates training of the agent. Will display statistical
              information about the training process in the readout space.
          </p>
          <h6>
              8) Halt
          </h6>
          <p>
              Prematurely ends the current session of training or testing.
          </p>
          <h6>
              9) Test
          </h6>
          <p>
              Set the agent to perform in the environment exclusively according to the current policy and
              returns results.
          </p>
          <p>
              NOTE: It is advised that the user sets epsilon to zero during testing at this
              time. It will occasionally produce a bug otherwise. This will be fixed in a future patch.
          </p>
          <h6>
              10) Save Model
          </h6>
          <p>
              Saves the state of the current agent to a file.
          </p>
          <h6>
              11) Load Model
          </h6>
          <p>
              Opens a file selection window that allows the user to select an agent to be loaded. The agent
              must match the Agent/Environment combination selected during the Start Screen of the tab.
          </p>
          <h6>
              12) Reset
          </h6>
          <p>
              Sets the parameters and the agent state to the default.
          </p>
          <h6>
              13) Save Results
          </h6>
          <p>
              Opens a file selection window and allows the user to write a save file containing the results of
              training or testing an agent/environment interaction as a csv file.
          </p>
          <br>
          <h5>
              TABS AND BUTTONS AND VISUALS
          </h5>
          <h6>
              1) Reset
          </h6>
          <p>
              Sets the model to its opening default state.
          </p>
          <h6>
              2) Load Environment
          </h6>
          <p>
              Opens a file selection window that allows the user to load a custom built environment into the set
              of environments in the scrollable menu.
          </p>
          <h6>
              3) Load Agent
          </h6>
          <p>
              Opens a file selection window and allows the user to load a custom built agent in the set of agents
              in the scrollable menu.
          </p>
          <h6>
              4) Legend
          </h6>
          <p>
              MSE - Mean squared error for recording the loss of an agent/environment interaction
          </p>
          <p>
              Episode Reward - The resulting reward achieved during an episode
          </p>
          <p>
              Epsilon - The current epsilon value during training
          </p>
          <p>
              Display Episode - The current environment display episode number
          </p>
          <p>
              Display Step - The current environment display step number
          </p>
          <p>
              These contribute to a readout of the performance of an agent/environment interaction.
          </p>
          <h6>
              5) Toggle Display
          </h6>
          <p>
              Toggles if the environment will be displayed with the play and pause button
          </p>
          <h6>
              6) Save Graph
          </h6>
          <p>
              Saves the displayed graph to the user's system.
          </p>
          <h6>
              7) Toggle Legend
          </h6>
          <p>
              Clicking the legend on the graph can remove or add the legend's data points in the graph.
          </p>
          <h6>
              8) Toggle ratio aspect
          </h6>
          <p>
              Clicking the this will change the ratio aspect of the displayed environment.
          </p>
          <br>
          <h5>
              Advanced - API information:
          </h5>
          <h6>
              The Portal API requires methods for environments and agents as follows -
          </h6>
          <h6>
               Environment:
          </h6>
          <p>
              Must extend the environment abstract class and contain all abstract methods within and as set forth
              in the API documentation.
          </p>
          <h6>
              Agent:
          </h6>
          <p>
              Must extend the abstract modelBasedAgent or modelFreeAgent class from the Agents library or one of
              their child classes and contain all methods therein described by that class and the abstract Agent
              class and as set forth in the API documentation.
          </p>
<!--                the 'display episode speed' parameter can be changed mid-simulation to adjust how quickly the rendering runs. Keep in mind-->
<!--                that each loop is showing the most recent episode, so if you set it high it will often repeat the same episode and if set-->
<!--                low it will often skip episodes.-->


      </div>
    <!--needed for bootstrap-->
      <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.1/dist/umd/popper.min.js"
              integrity="sha384-SR1sx49pcuLnqZUnnPwx6FCym0wLsk5JZuNx2bPPENzswTNFaQU1RDvt3wT4gWFG"
              crossorigin="anonymous"></script>
      <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.min.js"
              integrity="sha384-j0CNLUeiqtyaRmlzUHCPZ+Gy5fQu0dQ6eZ/xAww941Ai1SxSY+0EQqNXNE6DZiVc"
              crossorigin="anonymous"></script>
  </body>

</html>